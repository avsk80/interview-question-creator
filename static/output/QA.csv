Question,Answer,Referred_page_nums
"1) {
    ""question"": ""What is the primary purpose of Apache Kafka in Big Data Analytics, especially in handling the high speeds that BI/Analytics environments require?"",
    ""answer_choices"": [""A) To support high throughput, scalable messaging system"", ""B) To provide a platform for distributed storage and processing"", ""C) To enable real-time data analytics"", ""D) To facilitate parallel jobs on clusters""]
}","Answer: Option A
Explanation: Apache Kafka is primarily used in Big Data Analytics to support a high throughput, scalable messaging system. It is designed to handle the high speeds that BI/Analytics environments require by efficiently processing and distributing large volumes of data in real-time. For example, Kafka can be used to collect and process streaming data from multiple sources simultaneously, ensuring data is delivered quickly and reliably to downstream systems for analysis.","[12, 12, 13, 13]"
"2) {
    ""question"": ""What are the fundamental components of Apache Pig in Big Data Analytics, considering the query examples inspired by Google's Dremel?"",
    ""answer_choices"": [""A) HDFS, MapReduce, YARN"", ""B) SQL, NoSQL, Python"", ""C) Pig Latin, Pig Interpreter, Spark"", ""D) TensorFlow, Keras, Scikit-learn""]
}","Answer: Option C
Explanation: The fundamental components of Apache Pig in Big Data Analytics are Pig Latin, Pig Interpreter, and Spark. Pig Latin is the scripting language used in Apache Pig for data processing, the Pig Interpreter converts Pig Latin scripts into MapReduce or Spark jobs, and Spark is a distributed computing system for big data processing. 

Example: In the context of the query examples inspired by Google's Dremel, Apache Pig would use Pig Latin scripts to process and analyze the data before converting them into MapReduce or Spark jobs for execution on a Hadoop cluster.","[12, 12, 14, 14]"
